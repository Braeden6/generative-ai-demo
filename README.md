# Reducing Hallucination in Generative AI: A Practical Demonstration

This demonstration showcases a method to increase the accuracy and trustworthiness of text generated by AI models. The approach helps to ground generated text in source material and reduce the amount of 'hallucination', or the generation of information not supported by the input data.

The demo uses a PDF file as the source material, which is downloaded from the internet. The text of the PDF is then converted into [embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture#:~:text=An%20embedding%20is%20a%20relatively,like%20sparse%20vectors%20representing%20words.) using OpenAI's APIs. With [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/), a library for efficient similarity search, the most relevant sections of the PDF are found based on your question. Finally, these sections are used to generate answers to your question using [GPT-4](https://openai.com/gpt-4).

The process involves various tools and techniques:

1) **[LangChain](https://python.langchain.com/docs/get_started/introduction.html)**: Simplifies the generative AI application building process.
2) **[FAISS](https://pypi.org/project/faiss/)**: Used for finding the most similar embeddings.
3) **Pickl**: Used for saving and loading embeddings, to avoid re-calculating them each time.
4) **OpenAI**: Provides APIs for generating embeddings and text with GPT-4.
5) **PyPDF**: Extracts text from the PDF.
6) **[Text Splitter](https://github.com/hwchase17/langchain/blob/master/langchain/text_splitter.py)**: Splits the PDF text into manageable chunks or pages for processing.
7) **Prompt Engineering**: Used to provide contextual cues to the GPT-4 model to improve the quality and relevance of its responses.

## Setting Up and Running the Demo

This guide will walk you through running the demonstration using VSCode and Jupyter Notebook. The end product will be a Jupyter Notebook which can answer questions based on the content of a PDF document. This approach can be used as a basis for developing more complex generative AI applications, like chatbots.

Before starting, ensure you have the necessary API keys and environment variables ready. You can use the provided `.env.example` file as a template to create your own `.env` file.

### Set up Virtual Environment
[Link to guide](https://code.visualstudio.com/docs/python/environments)
1) Ctrl + Shift + P
2) Environment Type: Venv
3) Select the virtual environment (hallucinationDemo or the name of the workspace folder)
4) Select the python interpreter
5) Add requirements.txt in drop down or run `pip install -r requirements.txt` in terminal

### Add Kernel to Jupyter Notebook
[Link to guide](https://code.visualstudio.com/docs/datascience/jupyter-kernel-management)
1) Open Jupyter Notebook File
2) Select the Kernel (top right corner)

### Accessing GPT-4
You can get an API key from [OpenAI](https://platform.openai.com/account/api-keys). New accounts receive a $15 credit, however, these accounts may not have access to GPT-4. Ensure that you have the necessary permissions before proceeding.

Remember, the goal of this demonstration is to show how you can reduce hallucinations and improve the accuracy of generative AI models. The approach outlined here could be used as a basis for building more complex applications, such as a question-answering chatbot grounded in source material, or an information retrieval system that maintains the traceability of its responses.

## Explore Further

You can adapt and expand upon this basic demonstration in various ways:

1. Try using different source documents to see how the choice of material impacts the quality and relevance of the generated responses.
2. Experiment with different settings and techniques for prompt engineering to increase quality of the responses of the GPT-4 model.
3. Explore how changing the size or scope of the text chunks impacts the performance of the FAISS similarity search.
4. Investigate other potential applications of this approach, such as fact-checking, information retrieval, or automated report generation.

Remember, the most powerful AI applications are often those that can learn from and adapt to a variety of different contexts and information sources. With the right tools and approaches, generative AI can be a powerful tool for making sense of the vast amount of text data available in the world today.