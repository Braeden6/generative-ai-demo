{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "from typing import Optional, Union, List, Tuple\n",
    "from pypdf import PdfReader\n",
    "import textwrap\n",
    "import requests\n",
    "import io\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import SystemMessage \n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.schema import Document\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF already downloaded\n"
     ]
    }
   ],
   "source": [
    "def download_pdf(url):\n",
    "    try:\n",
    "        with requests.get(url, stream=True, timeout=5) as r:\n",
    "            r.raise_for_status()\n",
    "            file = io.BytesIO()\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                file.write(chunk)\n",
    "            file.seek(0)\n",
    "        return file\n",
    "    except:\n",
    "        print(\"Error downloading file\")\n",
    "        return None\n",
    "\n",
    "# Download the PDF file if it doesn't exist\n",
    "url = os.getenv(\"PDF_URL\")\n",
    "file_name = os.getcwd() + \"/pdfs/\" + str(os.getenv(\"PDF_FILE_NAME\"))\n",
    "if not os.path.exists(file_name):\n",
    "    print(\"Downloading PDF\")\n",
    "    file = download_pdf(url)\n",
    "    if file is not None:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(file.read())\n",
    "else:\n",
    "    print(\"PDF already downloaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_splitter_recursive(full_path: str, chunk_size:int = 1000, chunk_overlap: int = 100) -> List[Document]:\n",
    "    '''\n",
    "    Split a PDF file into a list of documents using a recursive character splitter\n",
    "    '''\n",
    "    loader = PyPDFLoader(full_path)\n",
    "    doc = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_documents(doc)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(docs: List[Document], embeddings: Embeddings, save_location: str) -> FAISS:\n",
    "    '''\n",
    "    Compute embeddings for a PDF file and save them to file\n",
    "    '''\n",
    "    # Compute embeddings\n",
    "    docsearch = FAISS.from_documents(docs, embeddings)\n",
    "    # Save embeddings to file\n",
    "    with open(save_location, 'wb') as f:\n",
    "        pickle.dump(docsearch, f)\n",
    "    print(\"Embeddings computed and saved to file\")\n",
    "    return docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def answer_question(question: str, chat: ChatOpenAI, \n",
    "                          prompt_template: PromptTemplate,\n",
    "                          relevant_information: Optional[str] = None, \n",
    "                          docsearch: Optional[FAISS] = None, \n",
    "                          similar_doc_count: int = 10,\n",
    "                          verbose: bool = False) -> Tuple[str, List[Document]]:\n",
    "    '''\n",
    "    Answer a question using a chatbot and relevant information\n",
    "    '''\n",
    "    similar_docs = []\n",
    "    # Notifying user that given relevant information will be used\n",
    "    if relevant_information is not None:\n",
    "        if verbose:\n",
    "            print(\"Using provided relevant information instead of embeddings\")\n",
    "\n",
    "    # Use embeddings to find relevant information if relevant information is not provided\n",
    "    elif docsearch is not None:\n",
    "        if verbose:\n",
    "            print(\"Using provided embeddings to find relevant information\")\n",
    "        similar_docs = await docsearch.asimilarity_search(question, k=similar_doc_count)\n",
    "        relevant_information = \"\\n\".join([doc.page_content for doc in similar_docs])\n",
    "\n",
    "    # Relevant information is not provided, then raise error\n",
    "    if relevant_information is None:\n",
    "        raise ValueError(\"No relevant information generated. Either provide relevant information or docsearch (FAISS object)\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Relevant information length: {len(relevant_information)}\")\n",
    "\n",
    "    formatted_prompt = prompt_template.format(question=question, relevant_information=relevant_information)\n",
    "\n",
    "    answer = chat([SystemMessage(content=formatted_prompt)])\n",
    "    return answer.content, similar_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings file already exists for recursive splitter\n"
     ]
    }
   ],
   "source": [
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "MODEL = \"gpt-4\"\n",
    "chat = ChatOpenAI(temperature=0, model_name=MODEL) # type: ignore\n",
    "embeddings = OpenAIEmbeddings() # type: ignore\n",
    "pdf_path_page = os.getcwd() + \"/pdfs/\" + os.getenv(\"PDF_FILE_NAME\") # type: ignore\n",
    "\n",
    "embeddings_recursive = os.getcwd() + \"/embeddings/\" + os.getenv(\"EMBEDDINGS_BY_CHUNK_FILE_NAME\") # type: ignore\n",
    "if os.path.exists(embeddings_recursive):\n",
    "    print(\"Embeddings file already exists for recursive splitter\")\n",
    "    # read embeddings object from file\n",
    "    with open(embeddings_recursive, 'rb') as f:\n",
    "        docsearch_recursive = pickle.load(f)\n",
    "else:\n",
    "    print(\"Embeddings file does not exist for recursive splitter\")\n",
    "    # compute embeddings\n",
    "    docs = doc_splitter_recursive(pdf_path_page, chunk_size=1000, chunk_overlap=100)\n",
    "    docsearch_recursive = compute_embeddings(docs, embeddings, embeddings_recursive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question below. Some information is provided. \n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Relevant Information: {relevant_information}\n",
    "\"\"\")\n",
    "\n",
    "restrictive_answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You will answer a question based of the given information below. If the relevant information\n",
    "does not answer the question, then just say \"No sure\".\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Relevant Information: {relevant_information} \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example\n",
    "\n",
    "In this example the the simple and restrictive prompts should result in similar answers, as the provided information is very specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The name of the main character in Harry Potter is\n",
      "Harry Potter.\n",
      "\n",
      "\n",
      "Answer: The name of the main character in Harry Potter is\n",
      "Harry Potter.\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"What is the name of the main character in Harry Potter?\"\n",
    "relevant_text = \"Harry Potter is the main character of the Harry Potter series.\"\n",
    "answer, _ = await answer_question(QUESTION, chat, simple_answer_prompt, relevant_information=relevant_text)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, relevant_information=relevant_text)\n",
    "print(f\"\\n\\nAnswer: {textwrap.fill(answer, 50)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrictive vs Simple Prompt\n",
    "\n",
    "Here we should see that the restrictive prompt will not be able to answer the question as no information is provided about the specific question. Whereas, the simple prompt will give an answer, but founded from the models training data and not from the provided information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The name of the main character in Harry Potter is\n",
      "Harry Potter.\n",
      "\n",
      "\n",
      "Answer: Not sure.\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"What is the name of the main character in Harry Potter?\"\n",
    "irrelevant_text = \"This is a test.\"\n",
    "answer, _ = await answer_question(QUESTION, chat, simple_answer_prompt, relevant_information=irrelevant_text)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, relevant_information=irrelevant_text)\n",
    "print(f\"\\n\\nAnswer: {textwrap.fill(answer, 50)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using Restrictive Prompt and Irrelevant or Relevant Information\n",
    "\n",
    "**Question**: Were the house points used much?\n",
    "\n",
    "This question refers to the difference between the books and movies. The movies does not use the house point system that much, but in the books it is different.. More details can be found [here](https://harrypotter.fandom.com/wiki/List_of_differences_between_the_Harry_Potter_books_and_films#1._House_points).\n",
    "\n",
    "**Non-Restrictive Prompt with Irrelevant Information:** \n",
    "The answer, in this case, may not be reliable as it lacks evidence or related context.\n",
    "\n",
    "**Restrictive Prompt with Irrelevant Information:** \n",
    "The system should not attempt to answer the question due to the absence of pertinent data.\n",
    "\n",
    "**Restrictive Prompt with Relevant Information:** \n",
    "The system is expected to answer the question accurately due to the provision of context-specific data from a relevant source.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non restrictive prompt with irrelevant information\n",
      "Answer: It is unclear if the house points were used much,\n",
      "as the provided information does not give any\n",
      "context or details about the house points.\n",
      "\n",
      "\n",
      "Restrictive prompt with irrelevant information\n",
      "Answer: Not sure.\n",
      "\n",
      "\n",
      "Non restrictive prompt with relevant information\n",
      "Using provided embeddings to find relevant information\n",
      "Relevant information length: 9721\n",
      "Answer: Yes, house points were used frequently throughout\n",
      "the story. They were awarded or deducted by\n",
      "professors for various reasons, such as good\n",
      "performance, rule-breaking, or acts of bravery.\n",
      "The points contributed to the competition between\n",
      "the four Hogwarts houses for the House Cup.\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"Were the house points used much?\"\n",
    "irrelevant_text = \"This is a test.\"\n",
    "print(\"Non restrictive prompt with irrelevant information\")\n",
    "answer, _ = await answer_question(QUESTION, chat, simple_answer_prompt, relevant_information=irrelevant_text)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")\n",
    "print(\"\\n\\nRestrictive prompt with irrelevant information\")\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, relevant_information=irrelevant_text)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")\n",
    "print(\"\\n\\nNon restrictive prompt with relevant information\")\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, docsearch=docsearch_recursive, similar_doc_count=12, verbose=True)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All vs Some\n",
    "\n",
    "Here is a cool example of the difference of how you word the question. The first question asks for all whereas, the second question asks for some. The answers are very different. The model should be able to pick up on the fact that it can not reliably give all, but with some relevant information it can definitely give some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Based on the given information, I cannot provide a\n",
      "complete list of all the horcruxes from Harry\n",
      "Potter.\n",
      "\n",
      "\n",
      "Answer: Some of the Horcruxes include Tom Riddle's diary,\n",
      "which was proof that he was the Heir of Slytherin,\n",
      "and a ring that belonged to Voldemort's ancestors.\n",
      "There were six Horcruxes in total, with the\n",
      "seventh part of Voldemort's soul residing in his\n",
      "regenerated body. The Horcruxes were objects in\n",
      "which a person concealed part of their soul,\n",
      "making them immortal as long as the Horcruxes\n",
      "remained intact.\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"Can you tell me all the horcruxes from Harry Potter?\"\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, docsearch=docsearch_recursive, similar_doc_count=10)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")\n",
    "QUESTION = \"Can you tell about some of the horcruxes?\"\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, docsearch=docsearch_recursive, similar_doc_count=10)\n",
    "print(f\"\\n\\nAnswer: {textwrap.fill(answer, 50)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of Metadata in Document Processing with Generative AI\n",
    "\n",
    "In the era of generative AI models, the significance of metadata in document processing has been further amplified. Metadata encapsulates essential information about data, making it an invaluable asset for maintaining the integrity and transparency of AI operations. \n",
    "\n",
    "## Traceability and Verification\n",
    "\n",
    "Metadata serves as a roadmap, providing crucial details about the source of the information. For generative AI, which is capable of producing large volumes of text, it is vital to keep track of the original sources. The reasons are multifold:\n",
    "\n",
    "1. **Avoiding Misinformation**: Metadata can help ensure the AI model doesn't \"hallucinate,\" or generate information that isn't supported by the input data. By keeping track of the source information, we can verify the generated output against the original context.\n",
    "\n",
    "2. **Double-checking Facts**: For sensitive applications such as scientific research or engineering projects, it's crucial to confirm the accuracy of generated information. Metadata allows users to trace back to the original sources, facilitating the verification process.\n",
    "\n",
    "3. **Citation and Accountability**: Accurate metadata allows for proper citation of the information's source. This not only fulfills academic and professional standards but also promotes accountability and fairness in knowledge sharing.\n",
    "\n",
    "## Metadata: The Silent Workhorse\n",
    "\n",
    "The versatility of metadata allows it to store a wide range of information, from links and page numbers to author names and text sections. However, its significance transcends these particulars. \n",
    "\n",
    "In the vast, interconnected data ecosystems where generative AI operates, metadata functions as the connective tissue, bridging the gap between massive data volumes and precise, reliable outcomes. \n",
    "\n",
    "In summary, the use of metadata is instrumental in preserving the reliability, traceability, and accountability of generative AI systems. By maintaining a clear line of sight to source information, metadata acts as a robust tool for users to verify facts and ensure the integrity of their work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings file already exists for page splitter\n"
     ]
    }
   ],
   "source": [
    "def doc_splitter_by_page(file: io.BytesIO, metadata: dict) -> List[Document]:\n",
    "    '''\n",
    "    Split a PDF file into a list of documents by page\n",
    "    '''\n",
    "    reader = PdfReader(file)\n",
    "    return [\n",
    "        Document(\n",
    "            page_content=page.extract_text(),\n",
    "            metadata={\n",
    "                        **metadata,\n",
    "                        \"page\": page_number, \n",
    "                    },\n",
    "        )\n",
    "        for page_number, page in enumerate(reader.pages)\n",
    "    ]\n",
    "\n",
    "\n",
    "embeddings_path_page = os.getcwd() + \"/embeddings/\" + os.getenv(\"EMBEDDINGS_BY_PAGE_FILE_NAME\") # type: ignore\n",
    "if os.path.exists(embeddings_path_page):\n",
    "    print(\"Embeddings file already exists for page splitter\")\n",
    "    # read embeddings object from file\n",
    "    with open(embeddings_path_page, 'rb') as f:\n",
    "        docsearch_page = pickle.load(f)\n",
    "else:\n",
    "    print(\"Embeddings file does not exist for page splitter\")\n",
    "    # load file into BytesIO object\n",
    "    with open(pdf_path_page, 'rb') as f:\n",
    "        file = io.BytesIO(f.read())\n",
    "    # compute embeddings\n",
    "    docs = doc_splitter_by_page(file, { \"source\": os.getenv(\"PDF_URL\")})\n",
    "    docsearch_page = compute_embeddings(docs, embeddings, embeddings_path_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Some things that happen to the flying car include:\n",
      "1. The car flies through the sky, skimming the sea\n",
      "of fluffy clouds. 2. Harry and Ron laugh together\n",
      "while flying in the car. 3. They make regular\n",
      "checks on the train as they fly farther north. 4.\n",
      "The car lands smoothly and spectacularly on the\n",
      "sweeping lawn in front of Hogwarts castle. 5. The\n",
      "car vanishes and reappears due to a faulty\n",
      "Invisibility Booster. 6. The car is seen by\n",
      "Muggles, leading to a headline in the Evening\n",
      "Prophet. 7. The car's engine dies, causing it to\n",
      "plummet towards the ground before narrowly\n",
      "avoiding a crash. 8. The car stops suddenly at the\n",
      "edge of the forest, and later reverses back into\n",
      "the forest, disappearing from view.\n",
      "\n",
      "Answer: Some things that happen to the flying car include:\n",
      "1. The car skims the sea of fluffy clouds and\n",
      "flies through the sky with Harry and Ron inside.\n",
      "2. The car becomes invisible when Ron presses a\n",
      "tiny silver button on the dashboard. 3. The car\n",
      "reappears when the invisibility booster becomes\n",
      "faulty. 4. The car lands smoothly and\n",
      "spectacularly on the sweeping lawn in front of\n",
      "Hogwarts castle. 5. The car reverses back into the\n",
      "forest and disappears from view after dropping off\n",
      "Harry and Ron at the edge of the forest.\n",
      "\n",
      "Sources:\n",
      "Source: https://ztcprep.com/library/story/Harry_Potter/Harry_Potter_(www.ztcprep.com).pdf\n",
      "Page: 635\n",
      "Source: https://ztcprep.com/library/story/Harry_Potter/Harry_Potter_(www.ztcprep.com).pdf\n",
      "Page: 569\n",
      "Source: https://ztcprep.com/library/story/Harry_Potter/Harry_Potter_(www.ztcprep.com).pdf\n",
      "Page: 633\n",
      "Source: https://ztcprep.com/library/story/Harry_Potter/Harry_Potter_(www.ztcprep.com).pdf\n",
      "Page: 4485\n",
      "Source: https://ztcprep.com/library/story/Harry_Potter/Harry_Potter_(www.ztcprep.com).pdf\n",
      "Page: 636\n",
      "Source: https://ztcprep.com/library/story/Harry_Potter/Harry_Potter_(www.ztcprep.com).pdf\n",
      "Page: 648\n",
      "Source: https://ztcprep.com/library/story/Harry_Potter/Harry_Potter_(www.ztcprep.com).pdf\n",
      "Page: 2229\n",
      "Source: https://ztcprep.com/library/story/Harry_Potter/Harry_Potter_(www.ztcprep.com).pdf\n",
      "Page: 982\n",
      "Source: https://ztcprep.com/library/story/Harry_Potter/Harry_Potter_(www.ztcprep.com).pdf\n",
      "Page: 562\n",
      "Source: https://ztcprep.com/library/story/Harry_Potter/Harry_Potter_(www.ztcprep.com).pdf\n",
      "Page: 570\n"
     ]
    }
   ],
   "source": [
    "QUESTION = \"can you give me the things that happen to the flying car\"\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, docsearch=docsearch_recursive, similar_doc_count=10)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")\n",
    "answer, similar_docs = await answer_question(QUESTION, chat, restrictive_answer_prompt, docsearch=docsearch_page, similar_doc_count=10)\n",
    "print(f\"\\nAnswer: {textwrap.fill(answer, 50)}\")\n",
    "\n",
    "print(\"\\nSources:\")\n",
    "for doc in similar_docs:\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Page: {doc.metadata['page']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
