{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "from typing import Optional, Union, List, Tuple\n",
    "from pypdf import PdfReader\n",
    "import textwrap\n",
    "import requests\n",
    "import io\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import SystemMessage \n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.schema import Document\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url):\n",
    "    try:\n",
    "        with requests.get(url, stream=True, timeout=5) as r:\n",
    "            r.raise_for_status()\n",
    "            file = io.BytesIO()\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                file.write(chunk)\n",
    "            file.seek(0)\n",
    "        return file\n",
    "    except:\n",
    "        print(\"Error downloading file\")\n",
    "        return None\n",
    "\n",
    "# Download the PDF file if it doesn't exist\n",
    "url = os.getenv(\"PDF_URL\")\n",
    "file_name = os.getcwd() + \"/pdfs/\" + str(os.getenv(\"PDF_FILE_NAME\"))\n",
    "if not os.path.exists(file_name):\n",
    "    print(\"Downloading PDF\")\n",
    "    file = download_pdf(url)\n",
    "    if file is not None:\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(file.read())\n",
    "else:\n",
    "    print(\"PDF already downloaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_splitter_recursive(full_path: str, chunk_size:int = 1000, chunk_overlap: int = 100) -> List[Document]:\n",
    "    '''\n",
    "    Split a PDF file into a list of documents using a recursive character splitter\n",
    "    '''\n",
    "    loader = PyPDFLoader(full_path)\n",
    "    doc = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_documents(doc)\n",
    "\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(docs: List[Document], embeddings: Embeddings, save_location: str) -> FAISS:\n",
    "    '''\n",
    "    Compute embeddings for a PDF file and save them to file\n",
    "    '''\n",
    "    # Compute embeddings\n",
    "    docsearch = FAISS.from_documents(docs, embeddings)\n",
    "    # Save embeddings to file\n",
    "    with open(save_location, 'wb') as f:\n",
    "        pickle.dump(docsearch, f)\n",
    "    print(\"Embeddings computed and saved to file\")\n",
    "    return docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def answer_question(question: str, chat: ChatOpenAI, \n",
    "                          prompt_template: PromptTemplate,\n",
    "                          relevant_information: Optional[str] = None, \n",
    "                          docsearch: Optional[FAISS] = None, \n",
    "                          similar_doc_count: int = 10,\n",
    "                          verbose: bool = False) -> Tuple[str, List[Document]]:\n",
    "    '''\n",
    "    Answer a question using a chatbot and relevant information\n",
    "    '''\n",
    "    similar_docs = []\n",
    "    # Notifying user that given relevant information will be used\n",
    "    if relevant_information is not None:\n",
    "        if verbose:\n",
    "            print(\"Using provided relevant information instead of embeddings\")\n",
    "\n",
    "    # Use embeddings to find relevant information if relevant information is not provided\n",
    "    elif docsearch is not None:\n",
    "        if verbose:\n",
    "            print(\"Using provided embeddings to find relevant information\")\n",
    "        similar_docs = await docsearch.asimilarity_search(question, k=similar_doc_count)\n",
    "        relevant_information = \"\\n\".join([doc.page_content for doc in similar_docs])\n",
    "\n",
    "    # Relevant information is not provided, then raise error\n",
    "    if relevant_information is None:\n",
    "        raise ValueError(\"No relevant information generated. Either provide relevant information or docsearch (FAISS object)\")\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Relevant information length: {len(relevant_information)}\")\n",
    "\n",
    "    formatted_prompt = prompt_template.format(question=question, relevant_information=relevant_information)\n",
    "\n",
    "    answer = chat([SystemMessage(content=formatted_prompt)])\n",
    "    return answer.content, similar_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = \"gpt-3.5-turbo\"\n",
    "MODEL = \"gpt-4\"\n",
    "chat = ChatOpenAI(temperature=0, model_name=MODEL) # type: ignore\n",
    "embeddings = OpenAIEmbeddings() # type: ignore\n",
    "pdf_path_page = os.getcwd() + \"/pdfs/\" + os.getenv(\"PDF_FILE_NAME\") # type: ignore\n",
    "\n",
    "embeddings_recursive = os.getcwd() + \"/embeddings/\" + os.getenv(\"EMBEDDINGS_BY_CHUNK_FILE_NAME\") # type: ignore\n",
    "if os.path.exists(embeddings_recursive):\n",
    "    print(\"Embeddings file already exists for recursive splitter\")\n",
    "    # read embeddings object from file\n",
    "    with open(embeddings_recursive, 'rb') as f:\n",
    "        docsearch_recursive = pickle.load(f)\n",
    "else:\n",
    "    print(\"Embeddings file does not exist for recursive splitter\")\n",
    "    # compute embeddings\n",
    "    docs = doc_splitter_recursive(pdf_path_page, chunk_size=1000, chunk_overlap=100)\n",
    "    docsearch_recursive = compute_embeddings(docs, embeddings, embeddings_recursive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question below. Some information is provided. \n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Relevant Information: {relevant_information}\n",
    "\"\"\")\n",
    "\n",
    "restrictive_answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You will answer a question based of the given information below. If the relevant information\n",
    "does not answer the question, then just say \"No sure\".\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Relevant Information: {relevant_information} \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example\n",
    "\n",
    "In this example the the simple and restrictive prompts should result in similar answers, as the provided information is very specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"What is the name of the main character in Harry Potter?\"\n",
    "relevant_text = \"Harry Potter is the main character of the Harry Potter series.\"\n",
    "answer, _ = await answer_question(QUESTION, chat, simple_answer_prompt, relevant_information=relevant_text)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, relevant_information=relevant_text)\n",
    "print(f\"\\n\\nAnswer: {textwrap.fill(answer, 50)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrictive vs Simple Prompt\n",
    "\n",
    "Here we should see that the restrictive prompt will not be able to answer the question as no information is provided about the specific question. Whereas, the simple prompt will give an answer, but founded from the models training data and not from the provided information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"What is the name of the main character in Harry Potter?\"\n",
    "irrelevant_text = \"This is a test.\"\n",
    "answer, _ = await answer_question(QUESTION, chat, simple_answer_prompt, relevant_information=irrelevant_text)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, relevant_information=irrelevant_text)\n",
    "print(f\"\\n\\nAnswer: {textwrap.fill(answer, 50)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using Restrictive Prompt and Irrelevant or Relevant Information\n",
    "\n",
    "**Question**: Were the house points used much?\n",
    "\n",
    "This question refers to the difference between the books and movies. The movies does not use the house point system that much, but in the books it is different.. More details can be found [here](https://harrypotter.fandom.com/wiki/List_of_differences_between_the_Harry_Potter_books_and_films#1._House_points).\n",
    "\n",
    "**Non-Restrictive Prompt with Irrelevant Information:** \n",
    "The answer, in this case, may not be reliable as it lacks evidence or related context.\n",
    "\n",
    "**Restrictive Prompt with Irrelevant Information:** \n",
    "The system should not attempt to answer the question due to the absence of pertinent data.\n",
    "\n",
    "**Restrictive Prompt with Relevant Information:** \n",
    "The system is expected to answer the question accurately due to the provision of context-specific data from a relevant source.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Were the house points used much?\"\n",
    "irrelevant_text = \"This is a test.\"\n",
    "print(\"Non restrictive prompt with irrelevant information\")\n",
    "answer, _ = await answer_question(QUESTION, chat, simple_answer_prompt, relevant_information=irrelevant_text)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")\n",
    "print(\"\\n\\nRestrictive prompt with irrelevant information\")\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, relevant_information=irrelevant_text)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")\n",
    "print(\"\\n\\nNon restrictive prompt with relevant information\")\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, docsearch=docsearch_recursive, similar_doc_count=12, verbose=True)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All vs Some\n",
    "\n",
    "Here is a cool example of the difference of how you word the question. The first question asks for all whereas, the second question asks for some. The answers are very different. The model should be able to pick up on the fact that it can not reliably give all, but with some relevant information it can definitely give some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Can you tell me all the horcruxes from Harry Potter?\"\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, docsearch=docsearch_recursive, similar_doc_count=10)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")\n",
    "QUESTION = \"Can you tell about some of the horcruxes?\"\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, docsearch=docsearch_recursive, similar_doc_count=10)\n",
    "print(f\"\\n\\nAnswer: {textwrap.fill(answer, 50)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of Metadata in Document Processing with Generative AI\n",
    "\n",
    "In the era of generative AI models, the significance of metadata in document processing has been further amplified. Metadata encapsulates essential information about data, making it an invaluable asset for maintaining the integrity and transparency of AI operations. \n",
    "\n",
    "## Traceability and Verification\n",
    "\n",
    "Metadata serves as a roadmap, providing crucial details about the source of the information. For generative AI, which is capable of producing large volumes of text, it is vital to keep track of the original sources. The reasons are multifold:\n",
    "\n",
    "1. **Avoiding Misinformation**: Metadata can help ensure the AI model doesn't \"hallucinate,\" or generate information that isn't supported by the input data. By keeping track of the source information, we can verify the generated output against the original context.\n",
    "\n",
    "2. **Double-checking Facts**: For sensitive applications such as scientific research or engineering projects, it's crucial to confirm the accuracy of generated information. Metadata allows users to trace back to the original sources, facilitating the verification process.\n",
    "\n",
    "3. **Citation and Accountability**: Accurate metadata allows for proper citation of the information's source. This not only fulfills academic and professional standards but also promotes accountability and fairness in knowledge sharing.\n",
    "\n",
    "## Metadata: The Silent Workhorse\n",
    "\n",
    "The versatility of metadata allows it to store a wide range of information, from links and page numbers to author names and text sections. However, its significance transcends these particulars. \n",
    "\n",
    "In the vast, interconnected data ecosystems where generative AI operates, metadata functions as the connective tissue, bridging the gap between massive data volumes and precise, reliable outcomes. \n",
    "\n",
    "In summary, the use of metadata is instrumental in preserving the reliability, traceability, and accountability of generative AI systems. By maintaining a clear line of sight to source information, metadata acts as a robust tool for users to verify facts and ensure the integrity of their work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_splitter_by_page(file: io.BytesIO, metadata: dict) -> List[Document]:\n",
    "    '''\n",
    "    Split a PDF file into a list of documents by page\n",
    "    '''\n",
    "    reader = PdfReader(file)\n",
    "    return [\n",
    "        Document(\n",
    "            page_content=page.extract_text(),\n",
    "            metadata={\n",
    "                        **metadata,\n",
    "                        \"page\": page_number, \n",
    "                    },\n",
    "        )\n",
    "        for page_number, page in enumerate(reader.pages)\n",
    "    ]\n",
    "\n",
    "\n",
    "embeddings_path_page = os.getcwd() + \"/embeddings/\" + os.getenv(\"EMBEDDINGS_BY_PAGE_FILE_NAME\") # type: ignore\n",
    "if os.path.exists(embeddings_path_page):\n",
    "    print(\"Embeddings file already exists for page splitter\")\n",
    "    # read embeddings object from file\n",
    "    with open(embeddings_path_page, 'rb') as f:\n",
    "        docsearch_page = pickle.load(f)\n",
    "else:\n",
    "    print(\"Embeddings file does not exist for page splitter\")\n",
    "    # load file into BytesIO object\n",
    "    with open(pdf_path_page, 'rb') as f:\n",
    "        file = io.BytesIO(f.read())\n",
    "    # compute embeddings\n",
    "    docs = doc_splitter_by_page(file, { \"source\": os.getenv(\"PDF_URL\")})\n",
    "    docsearch_page = compute_embeddings(docs, embeddings, embeddings_path_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"can you give me the things that happen to the flying car\"\n",
    "answer, _ = await answer_question(QUESTION, chat, restrictive_answer_prompt, docsearch=docsearch_recursive, similar_doc_count=10)\n",
    "print(f\"Answer: {textwrap.fill(answer, 50)}\")\n",
    "answer, similar_docs = await answer_question(QUESTION, chat, restrictive_answer_prompt, docsearch=docsearch_page, similar_doc_count=10)\n",
    "print(f\"\\nAnswer: {textwrap.fill(answer, 50)}\")\n",
    "\n",
    "print(\"\\nSources:\")\n",
    "for doc in similar_docs:\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Page: {doc.metadata['page']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
